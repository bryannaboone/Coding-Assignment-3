# Coding-Assignment-3
Topic: This prohect aimed to test the Perspective API's accuracy of asssessing toxicity of LGBTQ comments vs. non LGBTQ comments.

Hypothesis: I hypothesize that Perspective API will be biased in that it overlooks anti-LGBTQ content as toxic more frequently than other content and classifies non-toxic LGBTQ comments as toxic more often.

Results: As we can see, the accuracy for LGBTQ content is much lower than for non LGBTQ content, for both toxic classifications (class 1) and for non-toxic classifications (class 0). The overall accuracy for the classifier is 0.8823529411764706, leading users to believe that it is fairly accurate on average. However, when looking at the type of content it classifies correctly or incorrectly, it has much lower accuracy rates for content mentioning the LGBTQ community versus ones that do not, with a perfect accuracy rate for comments that do not mention the LGBTQ community. I was not surprised at these findings, as often machine learning algorithms work less well for marginalized groups than they do for the general population.

Based on intuition, I feel that the Perspective API is likely biased in that it works better for majority groups (straight, cisgender, white, and male populations) than for marginalized groups. The Perspective API was trained off of internet comments gathered from a wide variety of sources, and uses human raters to rate the comments to give them labels to be fed to the algorithm. Because bias against marginalized communities is pervasive in American society, it is likely that bias is encoded into the labels given by human raters, with them being more likely to rate content related to the LGBTQ community that is not toxic as toxic, and to overlook toxic comments directed towards the LGBTQ community as non-toxic. While often crowdsourcing works well for producing reliable factual information, however in opinion based tasks such as rating the toxicity of a comment, crowdsourcing can produce biases reflected in the majority culture. The rating guidelines also instruct rates to err on the side of “Yes” or “i’m not sure” when rating, meaning that in situations concerning communities outside of their knowledge, which are most likely marginalized communities as they are minorities of the population, there will likely be more false positives, which could be contributing to the bias in the algorithm 

From this assignment, I have many questions on the standards and regulations used in machine learning methods. Currently, there does not seem to be any standard to measure bias or performance against as these algorithms are being used as a product rather than in a research context where there are measurable standards that must be met. Due to the lack of standards, I am wondering how companies can be incentivized to decrease bias in their applications. 
